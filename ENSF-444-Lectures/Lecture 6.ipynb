{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: \n",
    "Linear Models and Validation Metrics<br/>\n",
    "Jan 22, 2024\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: What is Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning is a type of machine learning that learns\n",
    "from labeled data, which consists of input/output pairs.\n",
    "- The output can be either a numerical value (regression) or a\n",
    "class (classification).\n",
    "- Supervised learning aims to make accurate predictions for new\n",
    "data that has not been seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Classification and Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification and regression are two types of supervised machine learning\n",
    "problems, where the goal is to `learn a mapping function` from input variables\n",
    "to output variables.\n",
    "- In `classification`, we want to assign a discrete label to an input, such as\n",
    "\"spam\" or \"not spam\" for an email.\n",
    "- In `regression`, we want to estimate a continuous value for an input, such as\n",
    "the price of a house based on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is that the output variable is\n",
    "- categorical for classification\n",
    "- continuous for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Linear Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are supervised learning algorithms that predict an\n",
    "output variable based on a `linear combination of input features`.\n",
    "They can be used for both regression and classification tasks,\n",
    "depending on whether the output variable is continuous or binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear combination example: TVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screen Size, Refresh Rate, Price\n",
    "\n",
    "`Screen Size` and `Refresh Rate` are Inputs<br/>\n",
    "`Price` is an Output\n",
    "\n",
    "y = mx+b\n",
    "m = slope, b = y-intercept\n",
    "\n",
    "`w_0*(Screen Size) + w_1*(Refresh Rate) + b = Price`\n",
    "\n",
    "Can make things simple, but what if the model is not Linear?\n",
    "- very basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Common Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression: Predicts a continuous output variable from one or more\n",
    "input features.\n",
    "    - For example, it can model how the height of a person varies with age, or\n",
    "how the price of a house depends on the size and location.\n",
    "- Logistic regression: Predicts a binary output variable from one or more input\n",
    "features.\n",
    "    - For example, it can estimate the probability of a patient having a heart\n",
    "disease or not, or the likelihood of a customer buying a product or not.\n",
    "8\n",
    "- Linear models are simple, interpretable, and fast to train. However, they\n",
    "may not perform well on complex or non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For regression, the general prediction formula for a linear model looks as\n",
    "follows:<br/>\n",
    "$\\hat{y}$ = 𝐰 ∙ 𝐱 + 𝑏<br/>\n",
    "    = [𝑤[0] ∗ 𝑥[0] + 𝑤[1] ∗ 𝑥[1] + . . . + 𝑤[𝑝] ∗ 𝑥[𝑝]] + 𝑏\n",
    "- Here, `𝐰 =[𝑤[0], 𝑤[1], ⋯, 𝑤[𝑝]]` and `𝐱 = [𝑥[0], 𝑥[1], ⋯, 𝑥[𝑝]]` are two\n",
    "vectors, and `\" ∙ \" is dot product`.\n",
    "10\n",
    "- Each `𝑥[0] to 𝑥[𝑝]` denotes the `features` \n",
    "(in this example, the number of features is p+1) of a single data point.\n",
    "- Also, `𝑤[0] to 𝑤[𝑝]` and `𝑏` are `parameters of the model` that are\n",
    "learned, and ^𝑦 is the prediction the model makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a dataset with a single feature, this is:<br/>\n",
    "$\\hat{y}$ = 𝑤[0] ∗ 𝑥[0] + 𝑏\n",
    "- This is the equation for a line.\n",
    "- Here, `𝑤_0 is the slope`, and `𝑏 is the y-axis offset` (intercept).\n",
    "- For more features, `w contains the slopes`\n",
    "along each feature axis.\n",
    "- Alternatively, you can think of the\n",
    "`predicted response` as being a \n",
    "`weighted sum of the input features`, with weights\n",
    "(which can be negative) given by the\n",
    "entries of w<br/>\n",
    "    <img src=\"./images/L6/L6-1.png\" alt=\"L6-1.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many different linear models for regression\n",
    "- The difference between these models lies in how the model parameters\n",
    "(w and b) are learned from the training data, and how model complexity\n",
    "can be controlled\n",
    "- Popular models used:\n",
    "    - Linear regression (ordinary least squares)\n",
    "    - [Ridge regression](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "    - [Lasso regression](https://scikit-learn.org/stable/modules/linear_model.html#lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge regression is also a linear model for regression, so it uses the same\n",
    "formula as ordinary least squares\n",
    "- For ridge regression, the coefficients (w) are not only chosen so that they\n",
    "predict well on the training data, but so they can also fit an additional constraint\n",
    "- The additional constraint is that the magnitude of coefficients must be as small\n",
    "as possible; all entries of w should be close to zero\n",
    "- The square of the [𝑙_2-norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) of the w's is defined as:<br/>\n",
    "    <img src=\"./images/L6/L6-3.png\" alt=\"L6-3.png\" width=\"600\"/>\n",
    "- The cost function to minimize becomes:<br/>\n",
    "    <img src=\"./images/L6/L6-4.png\" alt=\"L6-4.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having the coefficients close to zero means each feature should have as\n",
    "little effect on the outcome as possible (which translates to having a\n",
    "small slope), while still predicting well\n",
    "- This constraint is an example of what is called regularization\n",
    "- Regularization means explicitly restricting a model to avoid overfitting\n",
    "- Ridge regression uses L_2 regularization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
